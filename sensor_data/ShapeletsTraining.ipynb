{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-24T14:21:11.617937Z",
     "start_time": "2025-01-24T14:21:11.612698Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tslearn\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tslearn.shapelets import LearningShapelets, grabocka_params_to_shapelet_size_dict"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T14:24:26.599026Z",
     "start_time": "2025-01-24T14:24:25.764710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load data\n",
    "\n",
    "X_train = np.load('X_train.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "X_val = np.load('X_val.npy')\n",
    "\n",
    "y_train  = np.load('y_train.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "y_val = np.load('y_val.npy')"
   ],
   "id": "5fc5b0e9043341c9",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T14:25:11.618244Z",
     "start_time": "2025-01-24T14:25:11.610786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# determine total shapelets and their size r = size\n",
    "# get a dictionary of how many shapelets to be found for each shapelet length\n",
    "\n",
    "# Determine the number of time series in the training data\n",
    "n_ts = X_train.shape[0]  # Number of trials in the training data\n",
    "ts_sz = X_train.shape[1]  # Length of each time series (2400 as per instance_size)\n",
    "n_classes = 2  # Assuming you have two classes (e.g., ADL and fall)\n",
    "\n",
    "# Shapelet parameters\n",
    "l = 0.15  # Proportion of the time series length used for the smallest shapelet\n",
    "r = 3     # Maximum size of shapelets\n",
    "# Get the dictionary of shapelets to be found for each shapelet length\n",
    "shapelet_dict = grabocka_params_to_shapelet_size_dict(\n",
    "    n_ts=n_ts, ts_sz=ts_sz, n_classes=n_classes, l=l, r=r\n",
    ")\n",
    "\n",
    "# Display the shapelet dictionary\n",
    "print(shapelet_dict)"
   ],
   "id": "28e460aac33cbda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{360: 6, 720: 6, 1080: 6}\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-24T14:27:13.921374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "\n",
    "# Set the target size for one instance\n",
    "instance_size = 2400  # Number of rows per trial\n",
    "n_features = 6  # Number of features (sensor columns)\n",
    "\n",
    "# Take the labels for learning shapelets\n",
    "train_y_learning_shapelets = y_train\n",
    "\n",
    "# Create the shapelet learner class\n",
    "# Set weight_regularizer to avoid overfitting\n",
    "# Set scale to True for scaling the data\n",
    "trans = LearningShapelets(\n",
    "    weight_regularizer=0.001,\n",
    "    batch_size=256,\n",
    "    max_iter=1500,\n",
    "    total_lengths=5,\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.003),\n",
    "    scale=True\n",
    ")\n",
    "\n",
    "# Dictionaries for storing shapelets learning history\n",
    "shapelets_learned = {}\n",
    "lhist = {}\n",
    "\n",
    "# ndarrays for storing the transformed dataset\n",
    "train_shapelets = None\n",
    "valid_shapelets = None\n",
    "test_shapelets = None\n",
    "shapelets_learned_as_ts = None\n",
    "\n",
    "# Iterate through each sensor axis\n",
    "for SENSOR in range(n_features):\n",
    "    # Extract data for the current sensor\n",
    "    train_data_sensor = X_train[:, :, SENSOR]\n",
    "    val_data_sensor = X_val[:, :, SENSOR]\n",
    "    test_data_sensor = X_test[:, :, SENSOR]\n",
    "\n",
    "    print(f\"Now finding shapelets from sensor {SENSOR} ---- {datetime.datetime.now().time()}\")\n",
    "\n",
    "    # Fit the shapelet learner on the training data for the current sensor\n",
    "    trans.fit(train_data_sensor, train_y_learning_shapelets)\n",
    "\n",
    "    # Store the learned shapelets and history\n",
    "    shapelets_learned[f'{SENSOR}'] = trans.shapelets_\n",
    "    lhist[f'{SENSOR}'] = trans.history_\n",
    "\n",
    "    # Transform the datasets based on learned shapelets\n",
    "    if train_shapelets is None:\n",
    "        train_shapelets = trans.transform(train_data_sensor)\n",
    "        valid_shapelets = trans.transform(val_data_sensor)\n",
    "        test_shapelets = trans.transform(test_data_sensor)\n",
    "        shapelets_learned_as_ts = trans.shapelets_as_time_series_\n",
    "    else:\n",
    "        train_shapelets = np.hstack((train_shapelets, trans.transform(train_data_sensor)))\n",
    "        valid_shapelets = np.hstack((valid_shapelets, trans.transform(val_data_sensor)))\n",
    "        test_shapelets = np.hstack((test_shapelets, trans.transform(test_data_sensor)))\n",
    "        shapelets_learned_as_ts = np.vstack((shapelets_learned_as_ts, trans.shapelets_as_time_series_))\n",
    "\n",
    "print(\"DONE\", datetime.datetime.now().time())"
   ],
   "id": "e79d678e26da4dbc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now finding shapelets from sensor 0 ---- 19:57:13.938791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Senuli\\Desktop\\env\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Senuli\\Desktop\\env\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Senuli\\Desktop\\env\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Senuli\\Desktop\\env\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Senuli\\Desktop\\env\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Senuli\\Desktop\\env\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Senuli\\Desktop\\env\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train the model\n",
    "model = lgbm.LGBMClassifier(n_estimators=3500, random_state=8)\n",
    "model.fit(train_shapelets, train_y, eval_set=[(valid_shapelets, val_y)], eval_metric='logloss')\n",
    "\n",
    "# Make predictions on the validation set\n",
    "valid_predictions = model.predict(valid_shapelets)"
   ],
   "id": "b75085a075e5eeca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(val_y, valid_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)"
   ],
   "id": "83e7c6ef2d1c800f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ],
   "id": "9e7d0218bfe1023a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Print classification report\n",
    "report = classification_report(valid_y, valid_predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ],
   "id": "a2e2e3cbd3443e40"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
